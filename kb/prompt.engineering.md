> Prompt Engineering is the process of designing and refining inputs (prompts) to direct the behavior of generative AI models towards producing desired outcomes. This discipline combines aspects of linguistics, psychology, and data science to optimize interactions with AI.

Applying the **Deming Cycle**, also known as the **PDCA (Plan-Do-Check-Act) cycle**, to prompt engineering in generative AI provides a structured method for continuous improvement of prompts. This iterative process helps refine the effectiveness of prompts to elicit better outputs from AI models. Here’s how each phase of the Deming Cycle can be applied specifically to the process of prompt engineering:

1. **Plan**

In the planning phase, you define the objectives and the expected outcomes of your prompts. This involves:

- **Identifying the task**: Determine what you want the AI to achieve, whether it's generating text, creating images, answering questions, or solving problems.
- **Drafting initial prompts**: Based on your understanding of the AI's capabilities, draft initial prompts that you believe will effectively direct the AI to produce the desired outcomes.
- **Setting metrics for success**: Establish clear criteria for what success looks like in the responses generated by the AI.

2. **Do**

This phase involves implementing the planned prompts and observing the AI's responses. Actions in this phase include:

- **Testing the prompts**: Use the prompts with the AI model to generate outputs
- **Collecting data**: Gather the responses and any other relevant data that can help assess the effectiveness of the prompts.

3. **Check**

In the check phase, you evaluate the outputs based on the criteria set during the planning phase. This evaluation helps determine if the prompts are effective or need refinement. Activities include:

- **Analyzing responses**: Look at the AI’s outputs to determine how well they meet the defined success metrics.
- **Identifying patterns**: Note any patterns or consistent issues in the responses that may indicate problems with the prompts.

4. **Act**

Based on the insights gained from the check phase, make adjustments to improve the prompts. This could involve:

- **Refining prompts**: Modify the existing prompts based on the specific issues identified. This might mean making prompts more specific, adding missing context, or adjusting the language to be clearer.
- **Testing new strategies**: If certain prompts consistently fail to elicit the desired outputs, consider different approaches or structures for the prompts.
- **Documenting learnings**: Keep records of what changes have been made and how they affected the outcomes. This documentation will be valuable for future prompt engineering efforts.

---

## Tokens

- https://platform.openai.com/tokenizer
- https://github.com/openai/tiktoken

Tokens are the basic units of text processed by AI models. In most language models, including those based on transformers like GPT (Generative Pre-trained Transformer), text is broken down into tokens before processing. A token can be as small as a character, such as a letter or punctuation mark, or as large as a word or part of a word. Tokenization is the process of converting text into tokens.

### Why are Tokens Important?

1. Understanding Model Limits: Many language models, especially those based on transformers, have a maximum token limit per input. Knowing how to manage tokens effectively is crucial for optimizing prompt design and response generation.
2. Efficiency in Processing: Efficient token management can help in staying within the computational limits, reducing costs, and speeding up processing times.
3. Influence on Output: The choice of tokenization affects how the model understands and generates text, influencing everything from grammatical correctness to stylistic nuances.

### How to Best Use Tokens

1. Token Economy: Since language models have token limits, structuring prompts to use tokens economically can maximize the information conveyed without exceeding these limits.
2. Understanding Tokenization: Different models may use different tokenization schemes (e.g., byte-pair encoding, WordPiece). Understanding the tokenization method used by your model can help in crafting effective prompts.
3. Advanced Techniques: For complex tasks, consider using techniques like token weighting or attention manipulation to emphasize or de-emphasize certain parts of the input.

### Where to Apply Knowledge of Tokens

1. Large Document Processing: When working with documents that exceed the token limits of a model, understanding tokens can help in segmenting the text appropriately without losing context.
2. Multi-turn Dialogue Systems: In systems where there are multiple interactions, managing tokens across turns can prevent information loss and ensure coherent conversations.
3. Content Generation: In content creation, effective token management can ensure that the full scope of the content fits within the model's processing capabilities without truncating important information.

---

## Prompting Strategies
